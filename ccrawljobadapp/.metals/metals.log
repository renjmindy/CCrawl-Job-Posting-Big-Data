[0m2021.03.15 16:42:03 INFO  Started: Metals version 0.10.0 in workspace '/home/cjen/week10/ccrawljobadapp' for client vscode 1.54.2.[0m
[0m2021.03.15 16:42:04 INFO  time: initialize in 0.8s[0m
[0m2021.03.15 16:42:04 WARN  Build server is not auto-connectable.[0m
[0m2021.03.16 06:01:20 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/src/main/scala/example/Hello.scala[0m
[0m2021.03.16 06:01:23 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package example

object Hello extends Greeting with App {
  println(greeting)
}

trait Greeting {
  lazy val greeting: String = "hello"
}

[0m2021.03.16 06:01:26 INFO  time: code lens generation in 6.27s[0m
[0m2021.03.16 06:01:36 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/src/test/scala/example/HelloSpec.scala[0m
package example

import org.scalatest.flatspec.AnyFlatSpec
import org.scalatest.matchers.should.Matchers

class HelloSpec extends AnyFlatSpec with Matchers {
  "The Hello object" should "say hello" in {
    Hello.greeting shouldEqual "hello"
  }
}

[0m2021.03.16 06:09:25 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/src/main/scala/com/cjen/ccrawljobapp/Runner.scala[0m




















Mar 16, 2021 6:10:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 92
[0m2021.03.16 06:10:20 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/src/main/scala/com/cjen/ccrawljobadapp/Runner.scala[0m


























[0m2021.03.16 06:27:31 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/src/main/scala/com/cjen/ccrawljobadapp/Runner.scala[0m
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
import sbt._

object Dependencies {
  lazy val scalaTest = "org.scalatest" %% "scalatest" % "3.2.2"
}

[0m2021.03.16 07:45:11 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/Dependencies.scala[0m
[0m2021.03.16 07:45:18 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/plugins.sbt[0m



[0m2021.03.16 07:45:18 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/plugins.sbt[0m

[0m2021.03.16 07:45:18 INFO  skipping build import with status 'Requested'[0m



[0m2021.03.16 07:45:28 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/plugins.sbt[0m
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
Mar 16, 2021 8:06:26 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 202
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
[0m2021.03.16 08:06:36 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/build.sbt[0m
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "ccrawljobadapp",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

Mar 16, 2021 8:09:35 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 218
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
package com.cjen.ccrawljobadapp

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Common Crawl Job Ads")
      .getOrCreate()

    val sc = spark.sparkContext

    import spark.implicits._
    sc.setLogLevel("WARN")

    val wetSegments = "s3a://wet-segments/random-paths.txt"
    val wetS3Paths = sc
      .textFile(wetSegments)
      .map("s3a://commoncrawl/" + _)
      .collect()

    val inputFiles = wetS3Paths.mkString(",")

    // Do some unsightly hadoop configuration in order to use a custom delimter
    val delimiter = "WARC/1.0"
    val conf = new Configuration(sc.hadoopConfiguration)
    conf.set("textinputformat.record.delimiter", delimiter)
    val hadoopFile = sc.newAPIHadoopFile(
      inputFiles,
      classOf[TextInputFormat],
      classOf[LongWritable],
      classOf[Text],
      conf
    )

    val records = hadoopFile.map { case (longWritable, text) => text.toString }
    val jobAdsRdd = findJobAds(records)
    val jobAdsDf = jobAdsRdd.toDF().limit(100000)

    val outputBucket = "s3a://emr-output-revusf/jobads"
    jobAdsDf.write
      .format("csv")
      .option("compression", "gzip")
      .mode("overwrite")
      .save(outputBucket)
  }

  def findJobAds(records: RDD[String]): RDD[String] = {
    records
      .filter(record => {
        val lines = record.split("\n")
        val containsJobUri = lines
          .find(_.startsWith("WARC-Target-URI:"))
          .map(uriHeader => uriHeader.split(" "))
          .map(split => if (split.length == 2) split(1) else "")
          .map(uri => uri.contains("job"))

        containsJobUri.getOrElse(false)
      })
      .map(record => {
        val lines = record.split("\n")
        val textWithoutHeaders = lines.filter(l => {
          !l.startsWith("WARC") &&
            !l.startsWith("Content-Type:") &&
            !l.startsWith("Content-Length:") &&
            !l.trim().isEmpty()
        })
        textWithoutHeaders.mkString("")
      })
  }

  def withQualifications(jobAds: RDD[String]): RDD[String] = {
    jobAds
      .filter(ad => {
        val lowercase = ad.toLowerCase()
        lowercase.contains("requirement") ||
        lowercase.contains("skill") ||
        lowercase.contains("certification")
      })
  }
}
[0m2021.03.16 08:09:55 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/build.sbt[0m
[0m2021.03.16 08:11:29 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/build.sbt[0m
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.cjen"
ThisBuild / organizationName := "cjen"

lazy val root = (project in file("."))
  .settings(
    name := "ccrawljobadapp",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.16 08:11:32 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4871161258035027872/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.16 08:11:33 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/metals.sbt[0m
[0m2021.03.16 08:11:33 WARN  no build target for: /home/cjen/week10/ccrawljobadapp/project/project/metals.sbt[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.03.16 08:11:33 INFO  skipping build import with status 'Dismissed'[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.03.16 08:11:35 INFO  [info] welcome to sbt 1.4.9 (Private Build Java 1.8.0_265)[0m
[0m2021.03.16 08:11:38 INFO  [info] loading settings for project ccrawljobadapp-build-build-build from metals.sbt ...[0m
[0m2021.03.16 08:11:38 INFO  [info] loading project definition from /home/cjen/week10/ccrawljobadapp/project/project/project[0m
[0m2021.03.16 08:11:42 INFO  [info] loading settings for project ccrawljobadapp-build-build from metals.sbt ...[0m
[0m2021.03.16 08:11:42 INFO  [info] loading project definition from /home/cjen/week10/ccrawljobadapp/project/project[0m
[0m2021.03.16 08:11:44 INFO  [success] Generated .bloop/ccrawljobadapp-build-build.json[0m
[0m2021.03.16 08:11:44 INFO  [success] Total time: 2 s, completed Mar 16, 2021 8:11:44 AM[0m
[0m2021.03.16 08:11:45 INFO  [info] loading settings for project ccrawljobadapp-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.16 08:11:45 INFO  [info] loading project definition from /home/cjen/week10/ccrawljobadapp/project[0m
[0m2021.03.16 08:11:46 INFO  [success] Generated .bloop/ccrawljobadapp-build.json[0m
[0m2021.03.16 08:11:47 INFO  [info] compiling 1 Scala source to /home/cjen/week10/ccrawljobadapp/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.03.16 08:11:51 INFO  [info] done compiling[0m
[0m2021.03.16 08:11:51 INFO  [success] Total time: 6 s, completed Mar 16, 2021 8:11:51 AM[0m
[0m2021.03.16 08:11:53 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.16 08:11:53 INFO  [info] set current project to ccrawljobadapp (in build file:/home/cjen/week10/ccrawljobadapp/)[0m
[0m2021.03.16 08:11:57 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.16 08:11:57 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.16 08:11:58 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.16 08:11:58 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.16 08:11:58 INFO  [success] Total time: 5 s, completed Mar 16, 2021 8:11:58 AM[0m
[0m2021.03.16 08:11:58 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.16 08:11:59 INFO  time: ran 'sbt bloopInstall' in 26s[0m
[0m2021.03.16 08:11:59 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5799246594943746545/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading 2 projects from '/home/cjen/week10/ccrawljobadapp/.bloop'...
[0m[32m[D][0m Loading project from '/home/cjen/week10/ccrawljobadapp/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/cjen/week10/ccrawljobadapp/.bloop/root-test.json'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5799246594943746545/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5799246594943746545/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.16 08:12:00 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/cjen/.cache/metals/bsp.trace.json[0m
[0m2021.03.16 08:12:00 INFO  Attempting to connect to the build server...[0m
[0m2021.03.16 08:12:00 INFO  Attempting to connect to the build server...[0mStarting the bsp launcher for bloop...

Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3020560518472041835/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher94261592837502634/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3020560518472041835/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3020560518472041835/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.16 08:12:00 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/cjen/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher94261592837502634/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher94261592837502634/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.16 08:12:00 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/cjen/.cache/metals/bsp.trace.json[0m
[0m2021.03.16 08:12:00 INFO  time: Connected to build server in 0.68s[0m
[0m2021.03.16 08:12:00 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.16 08:12:00 INFO  time: Imported build in 0.14s[0m
[0m2021.03.16 08:12:03 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.16 08:12:06 INFO  time: indexed workspace in 6.5s[0m
[0m2021.03.16 08:12:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.16 08:12:07 INFO  compiling ccrawljobadapp-build (1 scala source)[0m
[0m2021.03.16 08:12:15 INFO  time: compiled ccrawljobadapp-build in 8.35s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.cjen"
ThisBuild / organizationName := "cjen"

lazy val root = (project in file("."))
  .settings(
    name := "ccrawljobadapp",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.16 08:12:21 INFO  time: compiled root in 13s[0m
[0m2021.03.16 08:12:23 INFO  time: code lens generation in 7.14s[0m
[0m2021.03.16 08:12:23 INFO  time: code lens generation in 15s[0m
